---
title: "reading data from web"
output: github_document
---

Setup and load packages
```{r, setup, echo = FALSE, results = FALSE, message = FALSE}
library(tidyverse)
library(rvest)
library(httr)

knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "right"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d

```

Import data

```{r}
nsduh_url = "http://samhda.s3-us-gov-west-1.amazonaws.com/s3fs-public/field-uploads/2k15StateFiles/NSDUHsaeShortTermCHG2015.htm"

nsduh_html = read_html(nsduh_url)
```

this imports every table stored as a list
first() shortcut to get first element in a list of things
slice to remove first row with the Note info
we extracted table elements from the html file
```{r}
marj_use_df <-
  nsduh_html |> 
  html_table() |> 
  first() |> 
  slice(-1)
```


Import Star Wars
```{r}
swm_url = "https://www.imdb.com/list/ls070150896/"

swm_html = 
  read_html(swm_url)
```


first try to select the titles; Use Selector Gadget!
```{r}
swm_title_vec = swm_html |> 
  html_elements(".lister-item-header a") |> 
  html_text()

swm_gross_rev_vec = swm_html |> 
  html_elements(".text-small:nth-child(7) span:nth-child(5)") |> 
  html_text()


swm_df = 
  tibble(
    title = swm_title_vec,
    swm_gross_rev_vec
  )
```

## APIs
sending request to API to get the data; good bc reproducible 
click on API on website and click csv and copy link
Get water data from NYC
content() strip out request info just want the data
```{r}
nyc_water_df = GET("https://data.cityofnewyork.us/resource/ia2d-e54m.csv") |> 
  content("parsed")
```

BRFSS data
downloads 1000 out of 134000 rows; that's all it allowed us to download
can adjust limit to get more of the rows
found instructions for getting more code from the website
limits on the data size are not uncommon
```{r}
brfss_df = GET("https://data.cdc.gov/resource/acme-vg9e.csv",
               query = list("$limit" = 5000)) |> 
  content()
```


don't just download export the csv from the website; better to use API for reproducibility and easiest to run with updated datasets each time


Pokemon gives you weird data rectangle 
```{r}
poke_df = GET("https://pokeapi.co/api/v2/pokemon/ditto") |> 
  content()
```





